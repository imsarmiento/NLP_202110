{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Search Engine. Binary Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\allan\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\allan\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\allan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xml.dom import minidom\n",
    "from xml.etree import cElementTree as ElementTree\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('d001', 'William Beaumont and the Human Digestion William Beaumont and the Human Digestion.  William Beaumont: Physiology of digestion Image Source.  On November 21, 1785, US-American surgeon William Beaumont was born. He became best known as “Father of Gastric Physiology” following his research on human digestion. William Beaumont was born in Lebanon, Connecticut and became a physician. He served as a surgeon’s mate in the Army during the War of 1812. He opened a private practice in Plattsburgh, New York, but rejoined the Army as a surgeon in 1819. Beaumont was stationed at Fort Mackinac on Mackinac Island in Michigan in the early 1820s when it existed to protect the interests of the American Fur Company. The fort became the refuge for a wounded 19-year-old French-Canadian fur trader named Alexis St. Martin when a shotgun went off by accident in the American Fur Company store at close range June 6th, 1822. St. Martin’s wound was quite serious because his stomach was perforated and several ribs were broken. Nobody really expected that the young man would survive but he really did. The skin around St. Martin’s wound fused to the hole in his stomach, leaving a permanent opening – a gastric fistula. [1] Beaumont quickly noticed that there was much research potential. Back then, not too much was known about the digestive system. In order to gain more information, Beaumont performed numerous experiments on St. Martin over a period of eight years. The experiments must have been really uncomfortable for the man, who was inserted bits of different foods tied to strings through the hole in his stomach, pulling them out periodically to observe digestion. Beaumont also removed gastric juice, examining it to better understand its nature. Beaumont became the “Father of Gastric Physiology” and his findings were published in the book “Experiments and Observations on the Gastric Juice and the Physiology of Digestion” in 1833. The work is now considered as the basis of much of the early knowledge on digestion. William Beaumont discovered that hydrochloric acid is the main chemical responsible for breaking down food and he suggested that another important digestive chemical, which is now known as pepsin. He suggested that digestion is a chemical process, not merely a mechanical one caused by stomach muscle movement. Also, Beaumont gave insights on how emotions, temperature, and physical activity can affect digestion. Beaumont’s famous patient, St. Martin, outlived the scientist even though his wound never completely healed. He had several children and died at the age of 83. [2] At yovisto, you may be interested in a video lecture on The Digestive System.')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def document_reader():\n",
    "    \"\"\"\n",
    "    This method reads the documents\n",
    "    :return: Dictionary of documents (di: content of document i)\n",
    "    \"\"\"\n",
    "    documents_path = os.path.join(os.getcwd(), 'docs/docs-raw-texts')\n",
    "    documentos = {}\n",
    "    for filename in os.listdir(documents_path):\n",
    "        file_path = os.path.join(documents_path, filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        title = xmldoc.getElementsByTagName('fileDesc')[0].attributes['title'].value\n",
    "        data = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        documentos[id] = (title + ' ' + data).replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "\n",
    "    return documentos\n",
    "documents = document_reader()\n",
    "print(list(documents.items())[0])\n",
    "\n",
    "counterRom = 0\n",
    "for id, doc in documents.items():\n",
    "    match = re.search(r'\\bFabrication\\b', doc)\n",
    "    if match:\n",
    "        counterRom += 1\n",
    "        \n",
    "print(counterRom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q01': ['Fabrication', 'music', 'instrument'], 'q02': ['famous', 'German', 'poetry'], 'q03': ['Romanticism'], 'q04': ['University', 'Edinburgh', 'research'], 'q06': ['bridge', 'construction'], 'q07': ['Walk', 'Fame', 'star'], 'q08': ['Scientists', 'worked', 'atomic', 'bomb'], 'q09': ['Invention', 'Internet'], 'q10': ['early', 'telecommunication', 'method'], 'q12': ['Who', 'explored', 'South', 'Pole'], 'q13': ['famous', 'member', 'Royal', 'Navy'], 'q14': ['Nobel', 'Prize', 'winning', 'invention'], 'q16': ['South', 'America'], 'q17': ['Edward', 'Teller', 'Marie', 'Curie'], 'q18': ['Computing', 'Language', 'programming', 'Artificial', 'Intelligence'], 'q19': ['William', 'Hearst', 'movie'], 'q22': ['How', 'Captain', 'James', 'Cook', 'become', 'explorer'], 'q23': ['How', 'Grace', 'Hopper', 'get', 'famous'], 'q24': ['Computers', 'Astronomy'], 'q25': ['WWII', 'aircraft'], 'q26': ['Literary', 'critic', 'Thomas', 'Moore'], 'q27': ['Nazis', 'confiscate', 'destroy', 'art', 'literature'], 'q28': ['Modern', 'Age', 'English', 'Literature'], 'q29': ['modern', 'Physiology'], 'q32': ['Roman', 'Empire'], 'q34': ['Scientists', 'contributed', 'photosynthesis'], 'q36': ['Aviation', 'pioneer', \"'\", 'publication'], 'q37': ['Gutenberg', 'Bible'], 'q38': ['Religious', 'belief', 'scientist', 'explorer'], 'q40': ['Carl', 'Friedrich', 'Gauss', 'influence', 'colleague'], 'q41': ['Personalities', 'Hannover'], 'q42': ['Skinner', \"'s\", 'experiment', 'operant', 'conditioning', 'chamber'], 'q44': ['Napoleon', \"'s\", 'Russian', 'Campaign'], 'q45': ['Friends', 'enemy', 'Napoleon', 'Bonaparte'], 'q46': ['First', 'woman', 'Nobel', 'Prize']}\n"
     ]
    }
   ],
   "source": [
    "def queries_reader():\n",
    "    \"\"\"\n",
    "    Reads the query\n",
    "    :return: Dictionary of documents (di: content of document i)\n",
    "    \"\"\"\n",
    "    queries_path = os.path.join(os.getcwd(), 'docs/queries-raw-texts')\n",
    "    queries = {}\n",
    "    queries_paths = os.listdir(queries_path)\n",
    "    queries_paths.sort()\n",
    "    #print(documents_paths)\n",
    "    for filename in queries_paths:\n",
    "        file_path = os.path.join(queries_path, filename)\n",
    "        #print(filename)\n",
    "        xmldoc = minidom.parse(file_path)\n",
    "        id = xmldoc.getElementsByTagName('public')[0].attributes['publicId'].value\n",
    "        query = next(ElementTree.parse(file_path).iter('raw')).text\n",
    "        queries[id] = query.replace(u'\\xa0', u' ').replace('\\n', ' ')\n",
    "    return queries\n",
    "\n",
    "queries = queries_reader()\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001: famous\n",
      "d002: famous\n",
      "d003: famous\n",
      "d004: famous\n",
      "d007: famous\n",
      "d014: famous\n",
      "d017: famous\n",
      "d018: famous\n",
      "d019: famous\n",
      "d020: famous\n",
      "d021: famous\n",
      "d023: famous\n",
      "d024: famous\n",
      "d029: famous\n",
      "d037: famous\n",
      "d040: famous\n",
      "d042: famous\n",
      "d043: famous\n",
      "d045: famous\n",
      "d046: famous\n",
      "d047: famous\n",
      "d050: famous\n",
      "d052: famous\n",
      "d055: famous\n",
      "d058: famous\n",
      "d061: famous\n",
      "d063: famous\n",
      "d064: famous\n",
      "d067: famous\n",
      "d070: famous\n",
      "d077: famous\n",
      "d079: famous\n",
      "d080: famous\n",
      "d083: famous\n",
      "d085: famous\n",
      "d101: famous\n",
      "d102: famous\n",
      "d103: famous\n",
      "d105: famous\n",
      "d106: famous\n",
      "d107: famous\n",
      "d117: famous\n",
      "d119: famous\n",
      "d121: famous\n",
      "d126: famous\n",
      "d128: famous\n",
      "d129: famous\n",
      "d134: famous\n",
      "d136: famous\n",
      "d139: famous\n",
      "d141: famous\n",
      "d142: famous\n",
      "d143: famous\n",
      "d144: famous\n",
      "d145: famous\n",
      "d149: famous\n",
      "d150: famous\n",
      "d152: famous\n",
      "d153: famous\n",
      "d158: famous\n",
      "d160: famous\n",
      "d161: famous\n",
      "d162: famous\n",
      "d163: famous\n",
      "d164: famous\n",
      "d166: famous\n",
      "d172: famous\n",
      "d173: famous\n",
      "d179: famous\n",
      "d180: famous\n",
      "d184: famous\n",
      "d185: famous\n",
      "d186: famous\n",
      "d188: famous\n",
      "d190: famous\n",
      "d191: famous\n",
      "d194: famous\n",
      "d196: famous\n",
      "d197: famous\n",
      "d198: famous\n",
      "d200: famous\n",
      "d202: famous\n",
      "d203: famous\n",
      "d204: famous\n",
      "d205: famous\n",
      "d210: famous\n",
      "d211: famous\n",
      "d214: famous\n",
      "d215: famous\n",
      "d216: famous\n",
      "d218: famous\n",
      "d219: famous\n",
      "d221: famous\n",
      "d222: famous\n",
      "d224: famous\n",
      "d225: famous\n",
      "d226: famous\n",
      "d227: famous\n",
      "d228: famous\n",
      "d229: famous\n",
      "d230: famous\n",
      "d232: famous\n",
      "d233: famous\n",
      "d234: famous\n",
      "d236: famous\n",
      "d237: famous\n",
      "d238: famous\n",
      "d243: famous\n",
      "d244: famous\n",
      "d245: famous\n",
      "d248: famous\n",
      "d249: famous\n",
      "d255: famous\n",
      "d263: famous\n",
      "d266: famous\n",
      "d273: famous\n",
      "d277: famous\n",
      "d278: famous\n",
      "d279: famous\n",
      "d281: famous\n",
      "d284: famous\n",
      "d286: famous\n",
      "d287: famous\n",
      "d290: famous\n",
      "d291: famous\n",
      "d293: famous\n",
      "d294: famous\n",
      "d297: famous\n",
      "d298: famous\n",
      "d300: famous\n",
      "d302: famous\n",
      "d304: famous\n",
      "d306: famous\n",
      "d307: famous\n",
      "d309: famous\n",
      "d313: famous\n",
      "d314: famous\n",
      "d316: famous\n",
      "d317: famous\n",
      "d318: famous\n",
      "d319: famous\n",
      "d327: famous\n",
      "d328: famous\n",
      "d330: famous\n",
      "-----------------------\n",
      "d013: member\n",
      "d015: member\n",
      "d026: member\n",
      "d028: member\n",
      "d033: member\n",
      "d042: member\n",
      "d051: member\n",
      "d059: member\n",
      "d060: member\n",
      "d070: member\n",
      "d075: member\n",
      "d094: member\n",
      "d099: member\n",
      "d122: member\n",
      "d123: member\n",
      "d124: member\n",
      "d126: member\n",
      "d130: member\n",
      "d136: member\n",
      "d142: member\n",
      "d148: member\n",
      "d150: member\n",
      "d157: member\n",
      "d159: member\n",
      "d164: member\n",
      "d165: member\n",
      "d171: member\n",
      "d174: member\n",
      "d183: member\n",
      "d221: member\n",
      "d222: member\n",
      "d237: member\n",
      "d241: member\n",
      "d249: member\n",
      "d257: member\n",
      "d262: member\n",
      "d263: member\n",
      "d269: member\n",
      "d271: member\n",
      "d274: member\n",
      "d280: member\n",
      "d281: member\n",
      "d285: member\n",
      "d286: member\n",
      "d288: member\n",
      "d292: member\n",
      "d297: member\n",
      "d305: member\n",
      "d309: member\n",
      "d312: member\n",
      "-----------------------\n",
      "d015: Royal\n",
      "d016: Royal\n",
      "d019: Royal\n",
      "d022: Royal\n",
      "d026: Royal\n",
      "d037: Royal\n",
      "d040: Royal\n",
      "d042: Royal\n",
      "d043: Royal\n",
      "d049: Royal\n",
      "d050: Royal\n",
      "d056: Royal\n",
      "d060: Royal\n",
      "d070: Royal\n",
      "d072: Royal\n",
      "d074: Royal\n",
      "d079: Royal\n",
      "d087: Royal\n",
      "d096: Royal\n",
      "d098: Royal\n",
      "d099: Royal\n",
      "d100: Royal\n",
      "d113: Royal\n",
      "d118: Royal\n",
      "d119: Royal\n",
      "d126: Royal\n",
      "d136: Royal\n",
      "d147: Royal\n",
      "d156: Royal\n",
      "d165: Royal\n",
      "d189: Royal\n",
      "d190: Royal\n",
      "d191: Royal\n",
      "d193: Royal\n",
      "d212: Royal\n",
      "d221: Royal\n",
      "d235: Royal\n",
      "d240: Royal\n",
      "d241: Royal\n",
      "d243: Royal\n",
      "d245: Royal\n",
      "d246: Royal\n",
      "d248: Royal\n",
      "d250: Royal\n",
      "d253: Royal\n",
      "d256: Royal\n",
      "d259: Royal\n",
      "d262: Royal\n",
      "d269: Royal\n",
      "d270: Royal\n",
      "d272: Royal\n",
      "d285: Royal\n",
      "d286: Royal\n",
      "d288: Royal\n",
      "d289: Royal\n",
      "d290: Royal\n",
      "d292: Royal\n",
      "d297: Royal\n",
      "d304: Royal\n",
      "d305: Royal\n",
      "d309: Royal\n",
      "d310: Royal\n",
      "d311: Royal\n",
      "d313: Royal\n",
      "d317: Royal\n",
      "-----------------------\n",
      "d049: Navy\n",
      "d055: Navy\n",
      "d056: Navy\n",
      "d194: Navy\n",
      "d205: Navy\n",
      "d219: Navy\n",
      "d239: Navy\n",
      "d258: Navy\n",
      "d271: Navy\n",
      "d276: Navy\n",
      "d277: Navy\n",
      "d303: Navy\n",
      "-----------------------\n",
      "['d001', 'd002', 'd003', 'd004', 'd007', 'd014', 'd017', 'd018', 'd019', 'd020', 'd021', 'd023', 'd024', 'd029', 'd037', 'd040', 'd042', 'd043', 'd045', 'd046', 'd047', 'd050', 'd052', 'd055', 'd058', 'd061', 'd063', 'd064', 'd067', 'd070', 'd077', 'd079', 'd080', 'd083', 'd085', 'd101', 'd102', 'd103', 'd105', 'd106', 'd107', 'd117', 'd119', 'd121', 'd126', 'd128', 'd129', 'd134', 'd136', 'd139', 'd141', 'd142', 'd143', 'd144', 'd145', 'd149', 'd150', 'd152', 'd153', 'd158', 'd160', 'd161', 'd162', 'd163', 'd164', 'd166', 'd172', 'd173', 'd179', 'd180', 'd184', 'd185', 'd186', 'd188', 'd190', 'd191', 'd194', 'd196', 'd197', 'd198', 'd200', 'd202', 'd203', 'd204', 'd205', 'd210', 'd211', 'd214', 'd215', 'd216', 'd218', 'd219', 'd221', 'd222', 'd224', 'd225', 'd226', 'd227', 'd228', 'd229', 'd230', 'd232', 'd233', 'd234', 'd236', 'd237', 'd238', 'd243', 'd244', 'd245', 'd248', 'd249', 'd255', 'd263', 'd266', 'd273', 'd277', 'd278', 'd279', 'd281', 'd284', 'd286', 'd287', 'd290', 'd291', 'd293', 'd294', 'd297', 'd298', 'd300', 'd302', 'd304', 'd306', 'd307', 'd309', 'd313', 'd314', 'd316', 'd317', 'd318', 'd319', 'd327', 'd328', 'd330', 'NADA QUE VER', 'd013', 'd015', 'd026', 'd028', 'd033', 'd042', 'd051', 'd059', 'd060', 'd070', 'd075', 'd094', 'd099', 'd122', 'd123', 'd124', 'd126', 'd130', 'd136', 'd142', 'd148', 'd150', 'd157', 'd159', 'd164', 'd165', 'd171', 'd174', 'd183', 'd221', 'd222', 'd237', 'd241', 'd249', 'd257', 'd262', 'd263', 'd269', 'd271', 'd274', 'd280', 'd281', 'd285', 'd286', 'd288', 'd292', 'd297', 'd305', 'd309', 'd312', 'NADA QUE VER', 'd015', 'd016', 'd019', 'd022', 'd026', 'd037', 'd040', 'd042', 'd043', 'd049', 'd050', 'd056', 'd060', 'd070', 'd072', 'd074', 'd079', 'd087', 'd096', 'd098', 'd099', 'd100', 'd113', 'd118', 'd119', 'd126', 'd136', 'd147', 'd156', 'd165', 'd189', 'd190', 'd191', 'd193', 'd212', 'd221', 'd235', 'd240', 'd241', 'd243', 'd245', 'd246', 'd248', 'd250', 'd253', 'd256', 'd259', 'd262', 'd269', 'd270', 'd272', 'd285', 'd286', 'd288', 'd289', 'd290', 'd292', 'd297', 'd304', 'd305', 'd309', 'd310', 'd311', 'd313', 'd317', 'NADA QUE VER', 'd049', 'd055', 'd056', 'd194', 'd205', 'd219', 'd239', 'd258', 'd271', 'd276', 'd277', 'd303', 'NADA QUE VER']\n"
     ]
    }
   ],
   "source": [
    "def tokenization(documentos):\n",
    "    \"\"\"\n",
    "    :param documentos: Receives a dictionary  \n",
    "    :return: dict with key id of documents/queries and value is an array of terms\n",
    "    \"\"\"\n",
    "    nltk_stop_words_en = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    p_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    word_tok = {key: nltk.word_tokenize(doc) for key, doc in documentos.items()}\n",
    "    word_tok_sw = {key: [token for token in doc if token not in nltk_stop_words_en] for key, doc in word_tok.items()}\n",
    "    # nltk_stemedList_en = {key: [p_stemmer.stem(word) for word in doc] for key, doc in word_tok_sw.items()}\n",
    "    nltk_lemmaList = {key: [wordnet_lemmatizer.lemmatize(word) for word in doc] for key, doc in word_tok_sw.items()}\n",
    "\n",
    "    return nltk_lemmaList\n",
    "\n",
    "tokenized_docs = tokenization(documents)\n",
    "tokenized_queries = tokenization(queries)\n",
    "\n",
    "\n",
    "# print((list(tokenized_docs.items())[0]))\n",
    "# for query in tokenized_queries.items():\n",
    "#     for token in query[1]:\n",
    "#         counter = 0\n",
    "#         for id, doc in documents.items():\n",
    "#             match = re.search(rf'\\b{token}\\b', doc)\n",
    "#             if match:\n",
    "#                 counter += 1\n",
    "#                 break\n",
    "#         if counter == 0:\n",
    "#             print(query[0])\n",
    "#             break\n",
    "            \n",
    "\n",
    "#'Walk'\n",
    "lists = []\n",
    "for token in tokenized_queries['q12']:\n",
    "    counter2 = 0\n",
    "    for id, doc in documents.items():\n",
    "        match = re.search(rf'\\b{token}\\b', doc)\n",
    "        if match:\n",
    "            lists.append(id)\n",
    "            counter2 += 1\n",
    "            print(f'{id}: {token}')\n",
    "    lists.append('NADA QUE VER')\n",
    "    print('-----------------------')\n",
    "        \n",
    "# print(counter2)\n",
    "print(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 20447\n",
      "Size of the matrix:  20447 x 331\n"
     ]
    }
   ],
   "source": [
    "def matrix_construction(tokenized_docs):\n",
    "    \"\"\"\n",
    "    :param tokenized_docs: dict with key id of documents and value an array of terms\n",
    "    :return: Matrix term-document t1 = [d1, d2, ..., dn], where di is 1 o 0\n",
    "    \"\"\"\n",
    "    term_document = {}\n",
    "    for id,doc in tokenized_docs.items():\n",
    "        id = int(id[-3:]) #paasa dnjk al entero njk.\n",
    "        for token in doc:\n",
    "            if token not in term_document:\n",
    "                term_document[token] = [0] * len(list(tokenized_docs.items())) \n",
    "            \n",
    "            term_document[token][id-1] = 1\n",
    "    \n",
    "    return term_document\n",
    "\n",
    "term_document = matrix_construction(tokenized_docs)\n",
    "\n",
    "print('Size of vocabulary:', len(list(term_document.items())))\n",
    "print('Size of the matrix: ',len(list(term_document.items())), 'x', len(list(term_document.items())[0][1]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: [token,vector]\n",
      "Row in the csv: ['William', '1000000000000010000000000001000000100000000000000000001100000000000010000000000000000001001100100100010001001010000000000000000010000001010000000010000000000000000000000000001000110000000011100000100000000000000100000000000000000100000000001000000000000100100000000100000111000000000000001010010000110000000011000000000100100000010']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def serialize_and_save(term_document):\n",
    "    \"\"\"\n",
    "    :param term_document: Matrix term-document t1 = [d1, d2, ..., dn], where di is 1 o 0\n",
    "    :return: csv with columns (token,vector) donde vector es el string concatenado de 0s y 1s de la funcion binary_search\n",
    "    \"\"\"\n",
    "    serialize = [['token','vector']]\n",
    "    \n",
    "    for token,arr in term_document.items():\n",
    "        row =  [token, \"\".join([str(val) for val in arr])]\n",
    "        serialize.append(row)\n",
    "    \n",
    "    with open('document_term_file.csv', mode='w', encoding='utf-8', newline='') as document_term_file:\n",
    "        term_document_writer = csv.writer(document_term_file, delimiter=',')\n",
    "        term_document_writer.writerows(serialize)\n",
    "\n",
    "    return serialize\n",
    "value = serialize_and_save(term_document)\n",
    "    \n",
    "print('columns: [token,vector]')\n",
    "print('Row in the csv:',value[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q01': ['Fabrication', 'music', 'instrument'], 'q02': ['famous', 'German', 'poetry'], 'q03': ['Romanticism'], 'q04': ['University', 'Edinburgh', 'research'], 'q06': ['bridge', 'construction'], 'q07': ['Walk', 'Fame', 'star'], 'q08': ['Scientists', 'worked', 'atomic', 'bomb'], 'q09': ['Invention', 'Internet'], 'q10': ['early', 'telecommunication', 'method'], 'q12': ['Who', 'explored', 'South', 'Pole'], 'q13': ['famous', 'member', 'Royal', 'Navy'], 'q14': ['Nobel', 'Prize', 'winning', 'invention'], 'q16': ['South', 'America'], 'q17': ['Edward', 'Teller', 'Marie', 'Curie'], 'q18': ['Computing', 'Language', 'programming', 'Artificial', 'Intelligence'], 'q19': ['William', 'Hearst', 'movie'], 'q22': ['How', 'Captain', 'James', 'Cook', 'become', 'explorer'], 'q23': ['How', 'Grace', 'Hopper', 'get', 'famous'], 'q24': ['Computers', 'Astronomy'], 'q25': ['WWII', 'aircraft'], 'q26': ['Literary', 'critic', 'Thomas', 'Moore'], 'q27': ['Nazis', 'confiscate', 'destroy', 'art', 'literature'], 'q28': ['Modern', 'Age', 'English', 'Literature'], 'q29': ['modern', 'Physiology'], 'q32': ['Roman', 'Empire'], 'q34': ['Scientists', 'contributed', 'photosynthesis'], 'q36': ['Aviation', 'pioneer', \"'\", 'publication'], 'q37': ['Gutenberg', 'Bible'], 'q38': ['Religious', 'belief', 'scientist', 'explorer'], 'q40': ['Carl', 'Friedrich', 'Gauss', 'influence', 'colleague'], 'q41': ['Personalities', 'Hannover'], 'q42': ['Skinner', \"'s\", 'experiment', 'operant', 'conditioning', 'chamber'], 'q44': ['Napoleon', \"'s\", 'Russian', 'Campaign'], 'q45': ['Friends', 'enemy', 'Napoleon', 'Bonaparte'], 'q46': ['First', 'woman', 'Nobel', 'Prize']}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('q01', ['Fabrication', 'music', 'instrument']) \n",
      "('q02', ['famous', 'German', 'poetry']) d293\n",
      "('q03', ['Romanticism']) d105,d147,d152,d283,d291,d318\n",
      "('q04', ['University', 'Edinburgh', 'research']) d286\n",
      "('q06', ['bridge', 'construction']) d26,d29,d69,d257,d297,d329\n",
      "('q07', ['Walk', 'Fame', 'star']) d4\n",
      "('q08', ['Scientists', 'worked', 'atomic', 'bomb']) \n",
      "('q09', ['Invention', 'Internet']) \n",
      "('q10', ['early', 'telecommunication', 'method']) d231\n",
      "('q12', ['Who', 'explored', 'South', 'Pole']) \n",
      "('q13', ['famous', 'member', 'Royal', 'Navy']) \n",
      "('q14', ['Nobel', 'Prize', 'winning', 'invention']) \n",
      "('q16', ['South', 'America']) d132,d150,d184,d229,d250,d277\n",
      "('q17', ['Edward', 'Teller', 'Marie', 'Curie']) d121,d271\n",
      "('q18', ['Computing', 'Language', 'programming', 'Artificial', 'Intelligence']) \n",
      "('q19', ['William', 'Hearst', 'movie']) d179\n",
      "('q22', ['How', 'Captain', 'James', 'Cook', 'become', 'explorer']) \n",
      "('q23', ['How', 'Grace', 'Hopper', 'get', 'famous']) \n",
      "('q24', ['Computers', 'Astronomy']) \n",
      "('q25', ['WWII', 'aircraft']) \n",
      "('q26', ['Literary', 'critic', 'Thomas', 'Moore']) \n",
      "('q27', ['Nazis', 'confiscate', 'destroy', 'art', 'literature']) \n",
      "('q28', ['Modern', 'Age', 'English', 'Literature']) \n",
      "('q29', ['modern', 'Physiology']) d37\n",
      "('q32', ['Roman', 'Empire']) d25,d31,d90,d139,d254\n",
      "('q34', ['Scientists', 'contributed', 'photosynthesis']) \n",
      "('q36', ['Aviation', 'pioneer', \"'\", 'publication']) \n",
      "('q37', ['Gutenberg', 'Bible']) d169\n",
      "('q38', ['Religious', 'belief', 'scientist', 'explorer']) \n",
      "('q40', ['Carl', 'Friedrich', 'Gauss', 'influence', 'colleague']) \n",
      "('q41', ['Personalities', 'Hannover']) \n",
      "('q42', ['Skinner', \"'s\", 'experiment', 'operant', 'conditioning', 'chamber']) \n",
      "('q44', ['Napoleon', \"'s\", 'Russian', 'Campaign']) \n",
      "('q45', ['Friends', 'enemy', 'Napoleon', 'Bonaparte']) \n",
      "('q46', ['First', 'woman', 'Nobel', 'Prize']) d94\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def binary_search(queries):\n",
    "    \"\"\"\n",
    "    This method prints tokenized the queries, and print the list of documents that a query appears completely\n",
    "    :param queries: dict with key the id of a query and value the query\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    with open('document_term_file.csv', 'r', encoding='utf-8', newline='') as document_term_file:\n",
    "        rows = csv.reader(document_term_file)\n",
    "        dict_with_vector = {}\n",
    "        idx = 0\n",
    "        for row in rows:\n",
    "            idx+=1\n",
    "            if idx == 1:\n",
    "                continue\n",
    "            token = row[0]\n",
    "            string = row[1]\n",
    "            vector = [int(letter) for letter in list(string)]\n",
    "            dict_with_vector[token] = vector\n",
    "        base = [1]*331\n",
    "        for query in queries:\n",
    "            result = [1] * 331\n",
    "            bad_result = [0] * 331\n",
    "            id_document = 0\n",
    "            documents = []\n",
    "            for token in query[1]:\n",
    "                id_document += 1\n",
    "                if token in dict_with_vector:\n",
    "                    documents.append(id_document)\n",
    "                    result = np.bitwise_and(dict_with_vector[token], result)\n",
    "                else:\n",
    "                    result = np.bitwise_and(bad_result, result)\n",
    "                \n",
    "            idDoc = 0\n",
    "            string = ''\n",
    "            for res in result:\n",
    "                idDoc += 1\n",
    "                if res == 1:\n",
    "                    string += 'd' + str(idDoc) + ','\n",
    "            print(query, string[:-1])\n",
    "            \n",
    "\n",
    "binary_search(list(tokenized_queries.items()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
